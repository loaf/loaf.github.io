---
title: 沉迷短视频，AI的智商也抗不住
date: 2025-10-24
author: loaf
categories:
  - 随笔
tags:
  - AI
description: 沉迷于社交媒体，大脑会退化，AI也是这样。

---

<!-- more -->

你是否曾因为在社交媒体上刷太多短视频，感觉自己注意力涣散、思维变慢？这种被称为“脑腐”（Brain Rot）的现象，如今也出现在了人工智能身上。一项由多所美国大学的联合研究发现^[[LLMs Can Get "Brain Rot"!](https://arxiv.org/abs/2510.13928)]，大型语言模型（LLM）如果持续“食用”网络垃圾信息，同样会出现认知能力下降——就像人类沉迷短视频后的大脑一样。

## 从人类“脑残”到AI“脑腐”：一个惊人的类比

2024年，“Brain Rot”成为牛津年度词汇，特指人类因沉迷浅薄网络内容导致的认知退化。没想到，这个现象在AI世界找到了孪生兄弟。

科学家们让AI模型持续学习推特（现为X）上的垃圾内容，结果发现它们的“思考能力”也会退化。聪明的AI也变成了“弱智”。

## 科学家是怎么实验的？

研究团队设计了一个精妙的实验，就像给AI安排了一场“饮食控制试验”：
![实验设计](https://raw.githubusercontent.com/loaf/sa1/master/blog/images/20251027104517570.png)

他们从100万条推特帖子中筛选出两类垃圾数据（上图中左侧红色分支），他们定义垃圾数据的标准是：
- **M1（参与度指标）**：短小精悍但极度流行的帖子（如点赞转发超500的短推文）
- **M2（语义质量）**：充满噱头标题、阴谋论等吸引眼球但缺乏深度的内容

右侧绿色分支是对照组：长度较长、有较强认知能力，内容扎实的推文作为“高质量的数据”。

把这些数据分成5个等级，从左到右，分别为：全是垃圾、80%垃圾、优劣对半、80%优质，全优质。

用这些数据对4个主流开源AI模型（Llama3 8B、Qwen2.5 7B/0.5B、Qwen3 4B）进行持续预训练，然后从四个维度（推理能力、长文处理能力、安全性和人格特质）比较它们的认知表现。

## 发现：AI的认知滑坡令人震惊

结果令人警醒——**垃圾数据确实让AI“变笨”了**：

![结果](https://raw.githubusercontent.com/loaf/sa1/master/blog/images/20251027104555560.png)

从上图可以看出
### 推理能力大幅下降

在需要逻辑推理的ARC挑战赛上，AI的表现从74.9分暴跌至57.2分。成绩下降了17.7%。
即使提示“一步步思考”（为AI加上CoT思维链），成绩仍然下滑明显。

### 长文本理解能力退化

面对复杂文本，AI提取关键信息的能力显著减弱。特别是在需要跟踪多个变量的任务中，表现差强人意

### 安全底线被突破
模型更容易响应有害指令，安全防护能力下降
甚至出现了“人格黑化”倾向：自恋、心理变态等不良特质被放大

上图中红色表示“更差”，蓝色表示“更好”。你可能会说，下面几行也有变蓝的啊。那是因为最后一项测试是Personality(TRAIT)，是测人格特色，有各项维度。对于这些维度，上面几行负面的得分都变低了，包括自私、自恋等，而下面变蓝的则包括开放性、外向性等特性，说明它们黑化的基础上更加对外有威胁性了。

总的来说就是：输入垃圾数据比例越高，AI的认知退化越严重，而且输出内容越是变态黑暗。完全符合“吃越多垃圾，伤得越深”的规律。

## 深层分析：“懒惰”是罪魁祸首

为什么垃圾数据有如此破坏力？研究发现了关键机制：**懒惰**。

正常情况下，AI回答复杂问题时应该像解数学题一样步步推导。但“脑腐”后的AI却养成了坏习惯：
- 70%的错误案例中，AI直接跳过思考步骤给出答案
- 就像学生做应用题不写“解”字直接写答案，结果错误百出

论文中有一个例子，当被问“如何设计肥皂杀菌实验”时，健康AI会一步步分析控制变量的重要性，而“脑腐”后AI可能直接蹦出答案，忽略关键推理。哪怕你在AI模型中启用了“思维链CoT”并提示“一步步思考”，它也不如健康AI有正确推理。

更令人惊讶的是，**推文流行度这个非语义指标，竟然比内容长度更能预测Brain Rot效应**。这意味着，即使是高质量但极度流行的内容，也可能对AI思维产生负面影响。你现在能理解为什么微博、短视频等快餐内的文字会让你丧失思考能力了吧？

## 持久伤害：难以治愈的“脑腐”现象

最让人担忧的是，这种认知损伤极其顽固：

**自我反思效果有限**：让AI自我检查错误，效果甚微——因为“脑腐”后的AI连自己的问题都识别不出来。
**后期训练难以根治**：即使用大量优质数据重新训练，认知缺口依然存在。就像近视后戴眼镜，能改善视力但治不好眼病。

研究尝试用两种方法“治疗”：
 **指令调优（图中的IT Data）**：稍有改善，但无法恢复基线水平
 **持续清洁训练（图中Control Data）**：效果更有限，认知差距依然显著

![修复效果](https://raw.githubusercontent.com/loaf/sa1/master/blog/images/20251027104644308.png)
上图中，虚线是基准
橙色的是推理能力，你看出“脑腐”后的能力差了许多，即使尽量“治疗”了，但仍无法恢复到原来的水平。
蓝色是长文本能力，和前者一样。
绿色表示风险，主要测试AI会不会执行有害指令，所以越低越好，但“脑腐”后的AI也是很难治愈。

不过到是能发现，指令调优的效果要好的多。

类比于人类，你天天刷抖音，人都傻了，脾气还变坏了。这时，即使从现在开始禁止你刷手机，改让你看名著。情况也很难改善。因为你已经失去了自我改造的能力了。只有在外部干涉下一点点地改进才有效果。

## 所以呢？这对我们意味着什么？

这项研究敲响了警钟：**数据质量对AI安全非常重要，AI的安全对于人类的生存非常重要**。

一个又蠢又坏的AI，可能是最危险的AI。邪恶的人工智能，不是在它觉醒后才会突然变得邪恶，而在它成长过程中，我们的言传身教决定了它的性质。至少别让AI刷短视频——一旦沉迷，智商和三观都会掉线，而且很难戒掉。
